{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eede71b",
   "metadata": {},
   "source": [
    "**extract embedding layers from**:\n",
    "- pre-trained TFAutoModelForSequenceClassification: Can't get the hidden layers)\n",
    "    - reference: https://huggingface.co/docs/transformers/v4.15.0/custom_datasets#sequence-classification-with-imdb-reviews\n",
    "- pre-trained TFBertModel/TFGPT2Model pre-trained : can access to hidden layers + [CLS] + customer info\n",
    "    - reference: ttps://colab.research.google.com/drive/1-JIJlao4dI-Ilww_NnTc0rxtp-ymgDgM?usp=sharing\n",
    "    - https://huggingface.co/docs/transformers/model_doc/gpt2\n",
    "    - https://github.com/huggingface/transformers/issues/11891\n",
    "    - huggingface top 10 models: https://www.sabrepc.com/blog/Deep-Learning-and-AI/top-10-hugging-face-models-for-tensorflow\n",
    "- transformers fine-tuned\n",
    "- language model with attention self-trained : can access to hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575358d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "from transformers import BertModel, BertConfig\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from transformers import GPT2Tokenizer, TFGPT2Model, GPT2Config #distGPT2\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108ecd45",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15b6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse(path):\n",
    "#   g = gzip.open(path, 'rb')\n",
    "#   for l in g:\n",
    "#     yield eval(l)\n",
    "\n",
    "# def getDF(path):\n",
    "#   i = 0\n",
    "#   df = {}\n",
    "#   for d in parse(path):\n",
    "#     df[i] = d\n",
    "#     i += 1\n",
    "#   return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "# df = getDF('./data/reviews_Pet_Supplies.json.gz') #1,235,316\n",
    "# metadata_df = getDF('./data/meta_Pet_Supplies.json.gz')\n",
    "# merged_df = pd.merge(df, metadata_df, on = 'asin', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e7e66f",
   "metadata": {},
   "source": [
    "## Prepare a dataset: AutoTokenizer, and convert `datasets.arrow_dataset.Dataset` to TF_DF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e65339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3541b780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.overall.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1acd160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df['reviewText'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ca3e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['overall', 'reviewText']].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed70788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(sen):\n",
    "#     return tokenizer(sen, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99ea321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_Review = df['reviewText'].map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb5d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token_dict, label in zip(tokenized_Review, df.overall):\n",
    "#     token_dict['label'] = label - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79138d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_Review[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56fdcf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data/tokenized_Review.pickle\", \"wb\") as handle:\n",
    "#     pickle.dump(tokenized_Review, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5401a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/tokenized_Review.pickle\", \"rb\") as handle:\n",
    "    tokenized_Review = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53030587",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81e73030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='tf')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d131f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(tokenized_Review, test_size=0.2, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06f10a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train = train.sample(frac = 0.01, random_state = 1234)\n",
    "sample_test = test.sample(frac = 0.01, random_state = 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f234e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = tokenized_Review.train_test_split(test_size=0.2, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c7c6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(list(sample_train))\n",
    "train = Dataset(pa.Table.from_pandas(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab657514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label'],\n",
       "    num_rows: 9883\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79dc4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = pd.DataFrame(list(sample_test))\n",
    "test = Dataset(pa.Table.from_pandas(new_df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee3b1ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label'],\n",
       "    num_rows: 2471\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da32520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = train.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = test.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d34fc1",
   "metadata": {},
   "source": [
    "## TFAutoModelForSequenceClassification, cant access the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ec1fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(train) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5, \n",
    "    num_warmup_steps=0, \n",
    "    num_train_steps=total_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5da6d1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_242', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                             output_hidden_states = True, \n",
    "                                                             num_labels=5)\n",
    "\n",
    "\n",
    "# isInstance of distilbert configuration class: DistilBertForSequenceClassification (DistilBERT model)\n",
    "\n",
    "# isInstance of albert configuration class: AlbertForSequenceClassification (ALBERT model)\n",
    "\n",
    "# isInstance of camembert configuration class: CamembertForSequenceClassification (CamemBERT model)\n",
    "\n",
    "# isInstance of xlm roberta configuration class: XLMRobertaForSequenceClassification (XLM-RoBERTa model)\n",
    "\n",
    "# isInstance of roberta configuration class: RobertaForSequenceClassification (RoBERTa model)\n",
    "\n",
    "# isInstance of bert configuration class: BertForSequenceClassification (Bert model)\n",
    "\n",
    "# isInstance of xlnet configuration class: XLNetForSequenceClassification (XLNet model)\n",
    "\n",
    "# isInstance of xlm configuration class: XLMForSequenceClassification (XLM model)\n",
    "\n",
    "# isInstance of flaubert configuration class: FlaubertForSequenceClassification (Flaubert model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e760121c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "353931b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,957,317\n",
      "Trainable params: 66,957,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e50137ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#     tf_train_set,\n",
    "#     validation_data=tf_validation_set,\n",
    "#     epochs=1,\n",
    "# ) #one batch took more than half hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5400da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[0].embeddings.activity_regularizer = regularizers.l2(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b17ab282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertMainLayer at 0x192dee584c0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e1f5e",
   "metadata": {},
   "source": [
    "## TFBertModel/TFGPT2Model pre-trained : can access to hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "594f4e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "configuration = BertConfig(output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model = TFBertModel.from_pretrained(\"bert-base-uncased\", config=configuration)\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6155354d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 12, 768), dtype=float32, numpy=\n",
       " array([[[-0.02207932,  0.07241669, -0.21231124, ...,  0.25583994,\n",
       "          -0.06988102, -0.05784186],\n",
       "         [-0.10386208,  0.62958056,  1.3882225 , ...,  0.2654632 ,\n",
       "           1.852907  ,  0.35319582],\n",
       "         [ 0.26801813,  0.83381635, -0.67421234, ...,  0.1436718 ,\n",
       "           1.358183  ,  0.5122263 ],\n",
       "         ...,\n",
       "         [ 0.91765064,  0.9559129 ,  0.10018361, ..., -0.18450573,\n",
       "           0.6329654 ,  0.11736821],\n",
       "         [-0.1476281 ,  0.09667887, -0.20371097, ...,  0.28010172,\n",
       "           0.36521253,  0.44875804],\n",
       "         [-0.38059896,  0.01264042, -0.15247092, ..., -0.19050437,\n",
       "           0.3529018 ,  0.13312998]]], dtype=float32)>,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[2][1:2] #(1, 12, 768) hidden 12 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a80696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at saved_gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "configuration = GPT2Config(output_hidden_states=True)\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', output_hidden_states=True)\n",
    "model = TFGPT2Model.from_pretrained('gpt2', config = configuration)\n",
    "model.save_pretrained(\"saved_gpt2\")\n",
    "new_model = TFGPT2Model.from_pretrained('saved_gpt2')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = new_model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c64df2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 10, 768), dtype=float32, numpy=\n",
       " array([[[ 3.5697792 , -2.658392  ,  1.6498376 , ..., -1.2772871 ,\n",
       "          -1.2973205 ,  0.3542018 ],\n",
       "         [ 1.2458534 , -1.5179596 , -0.6840469 , ..., -0.03568979,\n",
       "           0.2956106 , -1.1468014 ],\n",
       "         [-0.86415637, -0.38782948, -0.26648208, ...,  0.86999714,\n",
       "           1.5778211 , -0.9438042 ],\n",
       "         ...,\n",
       "         [-0.55909264,  0.6272529 ,  0.45439622, ..., -0.5477285 ,\n",
       "          -0.48422644, -0.04893164],\n",
       "         [-0.2099823 , -0.49616966, -1.2750123 , ..., -1.1759973 ,\n",
       "           0.05631482, -0.07708734],\n",
       "         [ 0.52308846,  0.35600227, -0.3611122 , ..., -0.25321734,\n",
       "          -0.15091537,  0.11468042]]], dtype=float32)>,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[2][1:2] #(1, 10, 768) hidden 10 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ce269",
   "metadata": {},
   "source": [
    "## transformers fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0d01ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82fb7dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pytorch ----------------------\n",
    "\n",
    "# training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
    "\n",
    "# metric = load_metric(\"accuracy\")\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\") \n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c4b2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tensorflow -------------------------\n",
    "\n",
    "output_hidden_state = True\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                             output_hidden_states = True, \n",
    "                                                             num_labels=5)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "new_model = model.fit(tf_train_set, validation_data=tf_validation_set, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75e412b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,482,240\n",
      "Trainable params: 109,482,240\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667c106",
   "metadata": {},
   "source": [
    "## language model with attention self-trained : can access to hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991bf623",
   "metadata": {},
   "source": [
    "glue/sst2: General Language Understanding Evaluation benchmark (The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. )\n",
    "\n",
    "https://huggingface.co/transformers/v3.3.1/task_summary.html#sequence-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chewy_seo",
   "language": "python",
   "name": "chewy_seo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
